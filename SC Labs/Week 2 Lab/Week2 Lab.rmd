---
title: "Week 2 Lab"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

> **What this lab covers:** Probability distributions, the Central Limit
> Theorem, Normal distribution calculations, Poisson problems, Bayes'
> Theorem, and conditional probability. All of these are foundational
> concepts you'll use throughout the module.

# 1. Probability â€” Card Drawing

## The scenario

Imagine you have a **Magic: The Gathering** deck of **60 cards**:

- **20** are "land" cards
- **40** are non-land cards

You draw **7 cards** at random (like dealing a hand). We want to know
the probability of getting various numbers of land cards.

## Why is this NOT a Binomial distribution?

You might think: "Each card is either land or not-land, so it's
Binomial!" But there's a catch:

- **Binomial** requires each draw to be **independent** â€” the
  probability of drawing a land card must stay the same every time.
- But here we're drawing **without replacement** â€” once you draw a
  card, it's gone from the deck. So drawing one card *changes* the
  probability for the next draw.

This situation uses the **Hypergeometric distribution** instead. Think
of it as "the Binomial's cousin for sampling without replacement."

## R functions for the Hypergeometric distribution

| Function | What it gives you |
|---|---|
| `dhyper(x, m, n, k)` | P(exactly x successes) â€” the probability of drawing exactly `x` land cards |
| `phyper(q, m, n, k)` | P(X â‰¤ q) â€” the cumulative probability (at most `q` land cards) |

Where:

- `m` = number of "success" items in the population (**20** land cards)
- `n` = number of "failure" items (**40** non-land cards)
- `k` = number of draws (**7** cards)

```{r hypergeometric}
# Hypergeometric distribution: 60 cards total, 20 land, 40 non-land, draw 7

# i. Probability of exactly three land cards
p_three_land <- dhyper(3, m = 20, n = 40, k = 7)
cat("P(exactly 3 land cards) =", p_three_land, "\n")

# ii. Probability of more than three land cards
p_more_than_three <- 1 - phyper(3, m = 20, n = 40, k = 7)
cat("P(more than 3 land cards) =", p_more_than_three, "\n")

# iii. Probability of between two and four land cards (inclusive)
p_two_to_four <- sum(dhyper(2:4, m = 20, n = 40, k = 7))
cat("P(between 2 and 4 land cards) =", p_two_to_four, "\n")
# Alternative using phyper:
p_two_to_four_alt <- diff(phyper(c(1, 4), m = 20, n = 40, k = 7))
cat("P(between 2 and 4 land cards) [alt] =", p_two_to_four_alt, "\n")

# Expected number of land cards
# For hypergeometric: E[X] = n * K / N = 7 * 20 / 60
expected_land <- 7 * 20 / 60
cat("Expected number of land cards =", expected_land, "\n")
```

### How to interpret the results

- **P(exactly 3 land):** The most common outcome â€” you'll get exactly 3
  lands in about 26% of hands.
- **P(more than 3 land):** About 17% of the time you'll "flood" with
  too many lands.
- **P(between 2 and 4 land):** The "sweet spot" â€” about 74% of hands
  will give you a reasonable number of lands.
- **Expected value:** On average, you'll draw 7 Ã— (20/60) â‰ˆ **2.33**
  land cards per hand.

---

# 2. The Birthday Problem

## The scenario

How many people do you need in a room before there's a **50% chance**
that at least two of them share a birthday? Most people guess 100 or
more â€” the real answer is shockingly low!

## The key insight

It's easier to calculate the probability that **everyone** has a
*different* birthday, and then subtract from 1:

$$P(\text{at least one shared birthday}) = 1 - P(\text{all different})$$

For $N$ people and 365 possible birthdays:

- Person 1 can have any birthday: $\frac{365}{365}$
- Person 2 must differ from person 1: $\frac{364}{365}$
- Person 3 must differ from persons 1 & 2: $\frac{363}{365}$
- ...and so on.

So: $P(\text{all different}) = \frac{365}{365} \times \frac{364}{365}
\times \frac{363}{365} \times \ldots$

The code below computes this recursively:

```{r birthday_problem}
# Function to compute the probability that ALL n people have DIFFERENT birthdays
prob_all_diff <- function(N) {
  if (N <= 1) return(1)
  return(prob_all_diff(N - 1) * (365 - (N - 1)) / 365)
}

# Probability that at least two people share a birthday
prob_shared <- function(N) {
  return(1 - prob_all_diff(N))
}

# Generate probabilities for n = 1 to 60
n_values <- 1:60
p_values <- sapply(n_values, prob_shared)

# Find the smallest n for which the probability exceeds 0.5
n_threshold <- min(which(p_values >= 0.5))
cat("Smallest n where P(shared birthday) >= 0.5:", n_threshold, "\n")
cat("Probability at n =", n_threshold, ":", p_values[n_threshold], "\n")

# Plot
plot(n_values, p_values,
     type = "l", lwd = 2, col = "steelblue",
     xlab = "Number of people in room",
     ylab = "Probability of at least one shared birthday",
     main = "Birthday Problem",
     xlim = c(1, 60), ylim = c(0, 1))
abline(h = 0.5, col = "red", lty = 2, lwd = 2)
abline(v = n_threshold, col = "darkgreen", lty = 3, lwd = 1.5)
legend("bottomright",
       legend = c("P(shared birthday)", "p = 0.5", paste("n =", n_threshold)),
       col = c("steelblue", "red", "darkgreen"),
       lty = c(1, 2, 3), lwd = 2)
```

### The surprising answer

With just **23 people** in a room, the probability of a shared birthday
exceeds 50%! This feels wrong because we intuitively think about the
chance of someone sharing *our* birthday (which is low). But the
question is about *any* pair â€” and with 23 people there are
$\binom{23}{2} = 253$ possible pairs, so the chances add up quickly.

---

# 3. Random Variables â€” Expected Value and Variance

## What is a random variable?

A **random variable** is a quantity whose value depends on a random
process. For example, "the number of heads in 12 coin flips" is a
random variable.

Every random variable has two key properties:

- **Expected value** $E[X]$ â€” the long-run average (what you'd get if
  you repeated the experiment millions of times).
- **Variance** $\text{Var}(X)$ â€” how spread out the values are around
  the expected value.

## Three common distributions

### Binomial: $X \sim B(n=12, p=0.2)$

**What it models:** The number of successes in $n$ independent trials,
each with probability $p$ of success. Example: "Out of 12 coin flips
(with a biased coin that lands heads 20% of the time), how many heads?"

- $E[X] = np = 12 \times 0.2 = 2.4$ â€” On average, 2.4 successes out
  of 12 trials.
- $\text{Var}(X) = np(1-p) = 12 \times 0.2 \times 0.8 = 1.92$

### Normal: $A \sim N(\mu=10, \sigma=5)$

**What it models:** Continuous data that forms a bell curve. Example:
heights, test scores, measurement errors.

- $E[A] = \mu = 10$ â€” The centre of the bell curve.
- $\text{Var}(A) = \sigma^2 = 25$

### Poisson: $B \sim \text{Pois}(\lambda=4.5)$

**What it models:** The number of events in a fixed interval of time or
space. Example: "How many emails do I get per hour?"

- $E[B] = \lambda = 4.5$ â€” On average, 4.5 events per interval.
- $\text{Var}(B) = \lambda = 4.5$ â€” (For Poisson, the variance always
  equals the mean!)

```{r random_variables}
# Verify computations in R
n <- 12; p <- 0.2
cat("Binomial E[X] =", n * p, "  Var(X) =", n * p * (1 - p), "\n")

mu <- 10; sigma <- 5
cat("Normal E[A] =", mu, "  Var(A) =", sigma^2, "\n")

lambda <- 4.5
cat("Poisson E[B] =", lambda, "  Var(B) =", lambda, "\n")
```

---

# 4. Poisson Approximation to the Binomial

## The big idea

Sometimes we have a **Binomial** situation, but the number of trials
$n$ is large and the probability $p$ is small. In this case, the
Binomial is well-approximated by a **Poisson** distribution with
$\lambda = np$. This is useful because Poisson calculations can be
simpler.

**Rule of thumb:**

- Good approximation if $n \geq 20$ and $p \leq 0.05$
- Excellent approximation if $n \geq 100$ and $np \leq 10$

## Example: Defective screws

A factory produces screws, and **1% are defective** ($p = 0.01$). We
randomly select $n = 100$ screws. What's the probability that exactly
**5** are defective?

### Step 1: Exact answer using the Binomial

```{r binomial_exact}
# Exact probability using the Binomial distribution
# n = 100 screws, p = 0.01 (1% defective), find P(X = 5)
p_exact <- dbinom(5, size = 100, prob = 0.01)
cat("Exact P(X = 5) using Binomial =", p_exact, "\n")
```

### Step 2: Approximate answer using the Poisson

For the Binomial, the expected number of defectives is
$\mu = np = 100 \times 0.01 = 1$. We use this as our Poisson rate
$\lambda = 1$:

```{r poisson_approx}
# Approximate probability using the Poisson distribution
# lambda = np = 100 * 0.01 = 1
lambda_approx <- 100 * 0.01  # = 1
p_approx <- dpois(5, lambda = lambda_approx)
cat("Approximate P(X = 5) using Poisson (lambda =", lambda_approx, ") =", p_approx, "\n")

cat("\nComparison:\n")
cat("  Binomial (exact):  ", dbinom(5, 100, 0.01), "\n")
cat("  Poisson (approx):  ", dpois(5, 1), "\n")
cat("  Difference:        ", abs(dbinom(5, 100, 0.01) - dpois(5, 1)), "\n")
```

### Are they close?

Yes! The Poisson approximation (â‰ˆ 0.003066) is very close to the exact
Binomial answer (â‰ˆ 0.002898). With $n = 100$ and $np = 1$, we're well
within the "excellent approximation" zone.

---

# 5. Sampling Distribution â€” The Central Limit Theorem

## What is the Central Limit Theorem (CLT)?

The CLT is one of the most important results in all of statistics.
Here's what it says in plain English:

> **If you take many random samples from ANY distribution and compute
> the mean of each sample, those means will follow a Normal (bell
> curve) distribution â€” even if the original data is not Normal at
> all!**

Let's prove this to ourselves with a hands-on demonstration.

### Step 1: Create a non-Normal population

We generate 1,000,000 random numbers from a **Uniform distribution**
(every number between 0 and 1 is equally likely). This is definitely
NOT a bell curve â€” it's flat!

```{r}
a <- runif(10^6)
a
```

```{r}
hist(a, breaks = 20, main = "a", col = "lightblue")
```

> **What to see:** The histogram is **flat** â€” this is the Uniform
> distribution. Every value between 0 and 1 is equally likely. There's
> no peak, no bell shape.

```{r}
cat("Population mean:", mean(a), "\n")
cat("Standard deviation:", sd(a), "\n")
```

### Step 2: Take many random samples

We take **1,000 random samples**, each containing **100 numbers**, from
our population:

```{r}
b <- replicate(1000, sample(a, 100, replace = FALSE))
dim(b)
```

> **What `dim(b)` tells us:** The result is a 100 Ã— 1000 matrix â€” each
> **column** is one sample of 100 numbers. So `b[,1]` is the first
> sample, `b[,2]` is the second, etc.

### Step 3: Compute the mean of each sample

```{r}
# colMeans() computes the mean of each column (each sample)
sampleMean <- colMeans(b)
cat("Number of sample means:", length(sampleMean), "\n")
```

### Step 4: Plot the sample means â€” it looks Normal!

```{r}
hist(sampleMean, breaks = 20, main = "SampleMean", col = "lightblue",
     xlab = "Sample Mean", ylab = "Frequency")
```

> **The magic of the CLT:** Even though our original data was
> **flat/uniform**, the distribution of sample means is **bell-shaped**!
> This is exactly what the CLT predicts.

### Step 5: Check the numbers

The CLT also tells us the **spread** of the sampling distribution:

$$\text{SD of sample means} = \frac{\sigma_{\text{population}}}
{\sqrt{n}}$$

where $n$ is the sample size (100 in our case).

```{r}
cat("Mean of sampleMean:", mean(sampleMean), "\n")
cat("Standard deviation of sampleMean:", sd(sampleMean), "\n")
```

```{r}
cat("Theoretical SD of sampling distribution:", sd(a) / sqrt(100), "\n")
cat("Observed SD of sampleMean:              ", sd(sampleMean), "\n")
```

> **Confirmed!** The mean of the sample means is very close to the
> population mean, and the SD of the sample means matches
> $\sigma / \sqrt{n}$. The CLT works!

---

# 6. Plotting Expected Values â€” The Law of Large Numbers

## What is the Law of Large Numbers?

In plain English:

> **The more times you repeat an experiment, the closer the average
> result gets to the theoretical expected value.**

### Example: Flipping a coin

We know the probability of heads is 0.5, so the expected value of "the
proportion of heads" is 0.5. The plot below shows that as we increase
the number of flips, the observed proportion settles down toward 0.5:

```{r}
library(ggplot2)
```

```{r}
throw_a_coin <- function(N) {
  sample(c("H", "T"), size = N, replace = TRUE)
}

# Define trials
trials <- c(10, 20, 50, 70, 100, 200, 500, 800, 1000, 2000, 5000, 7000, 10000)

# Compute probability of heads for each trial
prob_heads <- sapply(trials, function(j) sum(throw_a_coin(j) == "H") / j)

# Create a data frame for plotting
df <- data.frame(trials = trials, prob_heads = prob_heads)

# Plot the results
ggplot(df, aes(x = trials, y = prob_heads)) +
  geom_point() +
  geom_line(alpha = 0.6) +
  scale_x_log10() +  # Log scale for x-axis
  geom_hline(yintercept = 0.5, color = "red") +
  labs(
    x = "Number of Trials",
    y = "Probability of Heads from Simulation",
    title = "Frequentist Probability of Heads"
  )
```

> **What to see:** With 10 flips, the proportion might be 0.3 or 0.7 â€”
> quite far from 0.5. But with 10,000 flips, it's very close to 0.5.
> That's the Law of Large Numbers in action!

### Your turn: Dice rolls

The expected value (long-run average) of a fair 6-sided die is:

$$E[X] = \frac{1+2+3+4+5+6}{6} = 3.5$$

For a 12-sided die: $E[X] = \frac{1+2+\ldots+12}{12} = 6.5$

The code below simulates 10,000 rolls of each die and plots the
**running average** (the average of all rolls so far) against the
number of rolls. Both should converge to their expected values:

```{r dice_simulation}
# Function to roll a 6-sided die N times and return the running average
roll_die <- function(N, sides = 6) {
  rolls <- sample(1:sides, size = N, replace = TRUE)
  return(cumsum(rolls) / seq_along(rolls))
}

set.seed(42)

# --- 6-sided die ---
N <- 10000
running_avg_6 <- roll_die(N, sides = 6)
df6 <- data.frame(trial = 1:N, running_avg = running_avg_6, die = "6-sided (E[X]=3.5)")

# --- 12-sided die ---
running_avg_12 <- roll_die(N, sides = 12)
df12 <- data.frame(trial = 1:N, running_avg = running_avg_12, die = "12-sided (E[X]=6.5)")

df_dice <- rbind(df6, df12)

ggplot(df_dice, aes(x = trial, y = running_avg, color = die)) +
  geom_line(alpha = 0.7) +
  geom_hline(yintercept = 3.5, color = "blue",  linetype = "dashed", linewidth = 0.8) +
  geom_hline(yintercept = 6.5, color = "red",   linetype = "dashed", linewidth = 0.8) +
  scale_x_log10() +
  labs(
    x = "Number of Trials (log scale)",
    y = "Running Average",
    title = "Convergence of Expected Value â€“ Dice Rolls",
    color = "Die Type"
  ) +
  theme_minimal()
```

> **What to see:** After just a few hundred rolls, the running average
> is already very close to the expected value. The 6-sided die
> converges to 3.5 and the 12-sided die converges to 6.5.

---

# 7. Normal Distribution Probabilities in R

## The key idea

For any Normal distribution, we can compute probabilities â€” i.e., "what
fraction of the bell curve falls to the left of a given value?"

## The `pnorm()` function â€” your best friend

`pnorm()` computes the **cumulative probability** $P(X \leq x)$ â€” the
area under the bell curve to the **left** of a value $x$.

**Syntax:**

``` r
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE)
```

| Argument | What it means |
|---|---|
| `q` | The value (cutoff point) you're interested in |
| `mean` | The centre of the bell curve (default: 0) |
| `sd` | The standard deviation â€” how wide the bell is (default: 1) |
| `lower.tail` | If `TRUE` (default): returns P(X â‰¤ q). If `FALSE`: returns P(X > q) |

### Cheat sheet for computing probabilities

| Probability you want | How to compute it in R |
|---|---|
| P(X < b) | `pnorm(b, mean, sd)` |
| P(X > b) | `pnorm(b, mean, sd, lower.tail = FALSE)` or `1 - pnorm(b, mean, sd)` |
| P(a < X < b) | `pnorm(b, mean, sd) - pnorm(a, mean, sd)` |

------------------------------------------------------------------------

## Example: Heights of Adult Women

Suppose women's heights follow a Normal distribution with:

- Mean (Î¼) = **65 inches**
- Standard deviation (Ïƒ) = **3 inches**

### Example 1: P(X < 68) â€” "What fraction of women are shorter than 68 inches?"

```{r}
# P(X < 68) when X ~ N(65, 3)
prob1 <- pnorm(68, mean = 65, sd = 3)
print(paste("P(X < 68) =", round(prob1, 4)))
```

**Alternative using the Z-transform:** We can convert to a "standard"
Normal (mean = 0, sd = 1) by computing how many standard deviations
68 is from the mean:

$$z = \frac{x - \mu}{\sigma} = \frac{68 - 65}{3} = 1$$

So $P(X < 68) = P(Z < 1)$:

```{r}
pnorm(1)
```

> **Interpretation:** About **84.13%** of women are shorter than 68
> inches. (Equivalently: 68 inches is 1 standard deviation above the
> mean.)

### Example 2: P(X > 68) â€” "What fraction of women are TALLER than 68 inches?"

This is just 1 minus the previous answer (the "complement"):

```{r example2}
# Method 1: Use lower.tail = FALSE
prob2a <- pnorm(68, mean = 65, sd = 3, lower.tail = FALSE)

# Method 2: Use the complement rule
prob2b <- 1 - pnorm(68, mean = 65, sd = 3)

print(paste("P(X > 68) =", round(prob2a, 4)))
print(paste("Verification:", round(prob2b, 4)))
```

> **Interpretation:** About **15.87%** of women are taller than 68
> inches.

### Example 3: P(62 < X < 68) â€” "What fraction of women are between 62 and 68 inches?"

We compute: P(X < 68) âˆ’ P(X < 62) = the area *between* 62 and 68.

```{r example3}
# P(62 < X < 68) = P(X < 68) - P(X < 62)
prob_less_68 <- pnorm(68, mean = 65, sd = 3)
prob_less_62 <- pnorm(62, mean = 65, sd = 3)
prob3 <- prob_less_68 - prob_less_62

print(paste("P(X < 68) =", round(prob_less_68, 4)))
print(paste("P(X < 62) =", round(prob_less_62, 4)))
print(paste("P(62 < X < 68) =", round(prob3, 4)))
```

> **Interpretation:** About **68.27%** of women have heights between
> 62 and 68 inches. This makes sense! The **68-95-99.7 rule** says
> that about 68% of values fall within 1 standard deviation of the
> mean, and 62 to 68 is exactly Î¼ Â± Ïƒ = 65 Â± 3.

------------------------------------------------------------------------

## Visualising the Probabilities

The shaded areas on these plots represent the probabilities we just
computed. The area under the curve equals the probability:

```{r visualization, fig.width=10, fig.height=8}
# Set up the plotting area for 3 plots
par(mfrow = c(3, 1), mar = c(4, 4, 3, 2))

# Create a sequence of x values for plotting
x <- seq(55, 75, length.out = 1000)
y <- dnorm(x, mean = 65, sd = 3)

# Plot 1: P(X < 68)
plot(x, y, type = "l", lwd = 2, 
     main = "P(X < 68)", 
     xlab = "Height (inches)", 
     ylab = "Density",
     xlim = c(55, 75))
# Shade the area
x_fill <- seq(55, 68, length.out = 1000)
y_fill <- dnorm(x_fill, mean = 65, sd = 3)
polygon(c(55, x_fill, 68), c(0, y_fill, 0), col = "lightblue", border = NA)
abline(v = 68, col = "red", lwd = 2, lty = 2)
text(68, max(y) * 0.9, "x = 68", pos = 4, col = "red")

# Plot 2: P(X > 68)
plot(x, y, type = "l", lwd = 2, 
     main = "P(X > 68)", 
     xlab = "Height (inches)", 
     ylab = "Density",
     xlim = c(55, 75))
# Shade the area
x_fill <- seq(68, 75, length.out = 1000)
y_fill <- dnorm(x_fill, mean = 65, sd = 3)
polygon(c(68, x_fill, 75), c(0, y_fill, 0), col = "lightcoral", border = NA)
abline(v = 68, col = "red", lwd = 2, lty = 2)
text(68, max(y) * 0.9, "x = 68", pos = 2, col = "red")

# Plot 3: P(62 < X < 68)
plot(x, y, type = "l", lwd = 2, 
     main = "P(62 < X < 68)", 
     xlab = "Height (inches)", 
     ylab = "Density",
     xlim = c(55, 75))
# Shade the area
x_fill <- seq(62, 68, length.out = 1000)
y_fill <- dnorm(x_fill, mean = 65, sd = 3)
polygon(c(62, x_fill, 68), c(0, y_fill, 0), col = "lightgreen", border = NA)
abline(v = c(62, 68), col = "red", lwd = 2, lty = 2)
text(62, max(y) * 0.9, "x = 62", pos = 2, col = "red")
text(68, max(y) * 0.9, "x = 68", pos = 4, col = "red")

# Reset plotting parameters
par(mfrow = c(1, 1))
```

------------------------------------------------------------------------

## Your Turn: Test Scores

**Context:** Test scores follow a Normal distribution with:

- Mean (Î¼) = **500**
- Standard deviation (Ïƒ) = **100**

### Question 1: P(X < 650) â€” "What fraction of students score below 650?"

```{r question1}
# Calculate P(X < 650) when X ~ N(500, 100)
prob_q1 <- pnorm(650, mean = 500, sd = 100)
print(paste("P(X < 650) =", round(prob_q1, 4)))
```

> **Interpretation:** About **93.32%** of students score below 650.
> That means scoring 650 puts you in the top 7% â€” quite impressive!

### Question 2: P(X > 650) â€” "What fraction score ABOVE 650?"

```{r question2}
# Calculate P(X > 650) when X ~ N(500, 100)
# Method 1: lower.tail = FALSE
prob_q2a <- pnorm(650, mean = 500, sd = 100, lower.tail = FALSE)
# Method 2: complement
prob_q2b <- 1 - pnorm(650, mean = 500, sd = 100)

print(paste("P(X > 650) =", round(prob_q2a, 4)))
print(paste("Verification:", round(prob_q2b, 4)))
```

> **Interpretation:** Only about **6.68%** of students score above
> 650.

### Question 3: P(450 < X < 650) â€” "What fraction score between 450 and 650?"

```{r question3}
# Calculate P(450 < X < 650) when X ~ N(500, 100)
prob_q3 <- pnorm(650, mean = 500, sd = 100) - pnorm(450, mean = 500, sd = 100)
print(paste("P(450 < X < 650) =", round(prob_q3, 4)))
```

> **Interpretation:** About **62.47%** of students score between 450
> and 650. This range spans from 0.5 standard deviations below the
> mean to 1.5 above it.

### Question 4: Visualise these three probabilities

```{r question4_visualization, fig.width=10, fig.height=8}
# Set up plotting area for 3 plots
par(mfrow = c(3, 1), mar = c(4, 4, 3, 2))

# x range for test scores
x_ts <- seq(200, 800, length.out = 1000)
y_ts <- dnorm(x_ts, mean = 500, sd = 100)

# Plot 1: P(X < 650)
plot(x_ts, y_ts, type = "l", lwd = 2,
     main = "P(X < 650)  â€” Test Scores N(500, 100)",
     xlab = "Score", ylab = "Density",
     xlim = c(200, 800))
x_fill <- seq(200, 650, length.out = 1000)
polygon(c(200, x_fill, 650), c(0, dnorm(x_fill, 500, 100), 0),
        col = "lightblue", border = NA)
abline(v = 650, col = "red", lwd = 2, lty = 2)
text(650, max(y_ts) * 0.9, "x = 650", pos = 2, col = "red")

# Plot 2: P(X > 650)
plot(x_ts, y_ts, type = "l", lwd = 2,
     main = "P(X > 650)  â€” Test Scores N(500, 100)",
     xlab = "Score", ylab = "Density",
     xlim = c(200, 800))
x_fill <- seq(650, 800, length.out = 1000)
polygon(c(650, x_fill, 800), c(0, dnorm(x_fill, 500, 100), 0),
        col = "lightcoral", border = NA)
abline(v = 650, col = "red", lwd = 2, lty = 2)
text(650, max(y_ts) * 0.9, "x = 650", pos = 4, col = "red")

# Plot 3: P(450 < X < 650)
plot(x_ts, y_ts, type = "l", lwd = 2,
     main = "P(450 < X < 650)  â€” Test Scores N(500, 100)",
     xlab = "Score", ylab = "Density",
     xlim = c(200, 800))
x_fill <- seq(450, 650, length.out = 1000)
polygon(c(450, x_fill, 650), c(0, dnorm(x_fill, 500, 100), 0),
        col = "lightgreen", border = NA)
abline(v = c(450, 650), col = "red", lwd = 2, lty = 2)
text(450, max(y_ts) * 0.9, "x = 450", pos = 2, col = "red")
text(650, max(y_ts) * 0.9, "x = 650", pos = 4, col = "red")

# Reset
par(mfrow = c(1, 1))
```

------------------------------------------------------------------------

## Bonus Challenge

### 1. What score is the 90th percentile?

"90th percentile" means the score below which 90% of students fall.
`qnorm()` is the **inverse** of `pnorm()` â€” you give it a probability
and it returns the corresponding value.

### 2. What percentage of students score within 2 standard deviations of the mean?

"Within 2 SDs" means between $\mu - 2\sigma = 300$ and
$\mu + 2\sigma = 700$.

```{r bonus}
# 1. 90th percentile score
p90 <- qnorm(0.90, mean = 500, sd = 100)
cat("90th percentile score:", round(p90, 2), "\n")

# 2. Percentage within 2 standard deviations (i.e., between 300 and 700)
p_within_2sd <- pnorm(700, mean = 500, sd = 100) - pnorm(300, mean = 500, sd = 100)
cat("Percentage within 2 SDs (300 to 700):", round(p_within_2sd * 100, 2), "%\n")
```

### Answers

1. A student needs to score approximately **628.16** to be in the 90th
   percentile â€” meaning they scored higher than 90% of test-takers.

2. Approximately **95.45%** of students score within 2 standard
   deviations of the mean. This is the well-known **68-95-99.7 rule**
   (also called the "empirical rule"): ~68% within 1 SD, ~95% within
   2 SDs, ~99.7% within 3 SDs.

------------------------------------------------------------------------

## Quick Reference: R Functions for the Normal Distribution

| Function | What it does | Example |
|---|---|---|
| `dnorm(x, mean, sd)` | Height of the bell curve at point x (the **density**) | `dnorm(65, 65, 3)` |
| `pnorm(q, mean, sd)` | Area to the **left** of q: P(X â‰¤ q) | `pnorm(68, 65, 3)` |
| `qnorm(p, mean, sd)` | The value where the left area = p (**inverse** of pnorm) | `qnorm(0.975, 65, 3)` |
| `rnorm(n, mean, sd)` | Generate `n` random numbers from this Normal | `rnorm(100, 65, 3)` |

> **Memory aid:** **d**ensity, **p**robability (cumulative),
> **q**uantile (inverse), **r**andom. This d/p/q/r pattern works for
> ALL distributions in R (e.g., `dpois`, `ppois`, `qpois`, `rpois`).

---

# 8. Poisson Distribution Problems

## The scenario

Galway United football team scores an average of **2 goals per game**.
The number of goals follows a **Poisson distribution** with $\lambda =
2$.

We want to compute four probabilities:

1. **Exactly 1 goal** in the next game
2. **At least 1 goal** in the next game
3. **Exactly 1 goal** across the next **two** games
4. **Exactly 2 goals given at least 1 goal** (conditional probability)

### Key idea for question 3

A neat property of the Poisson: if goals in one game follow
$\text{Pois}(\lambda)$, then goals in TWO games follow
$\text{Pois}(2\lambda)$. So for two games, $\lambda = 2 \times 2 = 4$.

### Key idea for question 4

This is a **conditional probability**: $P(X = 2 \mid X \geq 1) =
\frac{P(X = 2)}{P(X \geq 1)}$. We already know how to compute both
the numerator and denominator.

```{r poisson_goals}
lambda_per_game <- 2

# 1. P(X = 1) for one game
p_one_goal <- dpois(1, lambda = lambda_per_game)
cat("1. P(exactly 1 goal in next game) =", p_one_goal, "\n")

# 2. P(X >= 1) = 1 - P(X = 0)
p_at_least_one <- 1 - dpois(0, lambda = lambda_per_game)
cat("2. P(at least 1 goal in next game) =", p_at_least_one, "\n")

# 3. One goal in the next TWO games
# For two games, lambda = 2 * 2 = 4
lambda_two_games <- 2 * lambda_per_game
p_one_goal_two_games <- dpois(1, lambda = lambda_two_games)
cat("3. P(exactly 1 goal in next two games) =", p_one_goal_two_games, "\n")

# 4. P(X = 2 | X >= 1) = P(X = 2) / P(X >= 1)
p_two_goals <- dpois(2, lambda = lambda_per_game)
p_cond <- p_two_goals / p_at_least_one
cat("4. P(X = 2 | X >= 1) =", p_cond, "\n")
```

### Answers explained

1. $P(X = 1) = e^{-2}\frac{2^1}{1!} \approx$ `r round(dpois(1,2), 4)`
   â€” about **27%** chance of exactly 1 goal. (The most common
   single outcome, though "0 goals" and "2 goals" are also quite
   likely.)

2. $P(X \ge 1) = 1 - e^{-2} \approx$ `r round(1-dpois(0,2), 4)` â€”
   about **86%** chance of at least 1 goal. (Only ~14% chance of a
   goalless game.)

3. Over two games, $\lambda = 4$. $P(X=1) = e^{-4}\frac{4^1}{1!}
   \approx$ `r round(dpois(1,4), 4)` â€” only about **7%** chance of
   exactly 1 goal across two entire games (quite unlikely since the
   expected total is 4).

4. $P(X=2 \mid X \ge 1) \approx$ `r round(dpois(2,2)/(1-dpois(0,2)),
   4)` â€” about **31%**. Given that at least one goal is scored, the
   chance of exactly 2 is about 31%.

---

# 9. Bayes' Theorem â€” The Most Surprising Result in Probability

## The scenario

Imagine:

- A rare disease affects **1% of the population** ($P(D) = 0.01$).
- A diagnostic test is **95% accurate**:
  - If you HAVE the disease, the test correctly says "positive" 95% of
    the time ($P(T \mid D) = 0.95$).
  - If you DON'T have the disease, the test correctly says "negative"
    95% of the time ($P(T^C \mid D^C) = 0.95$).
  - But this also means 5% of healthy people get a **false positive**
    ($P(T \mid D^C) = 0.05$).

**The question:** If your test comes back **positive**, what's the
probability you actually have the disease?

Most people guess around 95%. The real answer is shockingly low.

## Setting up the probability statements

| Symbol | Meaning | Value |
|---|---|---|
| $P(D)$ | Prevalence (chance of having the disease) | 0.01 |
| $P(D^C)$ | Chance of NOT having the disease | 0.99 |
| $P(T \mid D)$ | Sensitivity (true positive rate) | 0.95 |
| $P(T^C \mid D)$ | False negative rate | 0.05 |
| $P(T \mid D^C)$ | False positive rate | 0.05 |
| $P(T^C \mid D^C)$ | Specificity (true negative rate) | 0.95 |

## Applying Bayes' Theorem

We want $P(D \mid T)$ â€” the probability of disease GIVEN a positive
test. Bayes' Theorem says:

$$P(D \mid T) = \frac{P(T \mid D) \cdot P(D)}{P(T)}$$

But we need $P(T)$ â€” the overall probability of testing positive. We
get this using the **Law of Total Probability** (every positive test
comes from either a sick person or a healthy person):

$$P(T) = P(T \mid D) \cdot P(D) + P(T \mid D^C) \cdot P(D^C)$$
$$= 0.95 \times 0.01 + 0.05 \times 0.99 = 0.0095 + 0.0495 = 0.059$$

Now plug into Bayes:

$$P(D \mid T) = \frac{0.95 \times 0.01}{0.059} \approx 0.161$$

```{r bayes_theorem}
# Given values
P_D   <- 0.01   # P(Disease)
P_T_given_D  <- 0.95  # P(Positive test | Disease) = sensitivity
P_T_given_Dc <- 0.05  # P(Positive test | No disease) = false positive rate

# Law of total probability for P(T)
P_T <- P_T_given_D * P_D + P_T_given_Dc * (1 - P_D)
cat("P(Positive test) =", P_T, "\n")

# Bayes' Theorem
P_D_given_T <- (P_T_given_D * P_D) / P_T
cat("P(Disease | Positive test) =", round(P_D_given_T, 4), "\n")

cat("\nSurprising? Even with a 95% accurate test, only about",
    round(P_D_given_T * 100, 1), "% of people who test positive actually have the disease!\n")
cat("This is because the disease is rare (1%), so false positives dominate.\n")
```

### Why is the answer so low?

Think about it with concrete numbers. In a population of **10,000
people**:

- **100** have the disease â†’ **95** test positive (true positives)
- **9,900** don't have the disease â†’ **495** test positive (false
  positives!)

So out of $95 + 495 = 590$ positive tests, only $95$ actually have
the disease. That's $95/590 \approx 16.1\%$.

> **The lesson:** When a disease is rare, even a highly accurate test
> produces more false positives than true positives!

### How does accuracy affect the result?

Let's plot $P(D \mid T)$ for different test accuracies:

```{r bayes_plot}
# Plot P(D|T) as a function of test accuracy (sensitivity = specificity = accuracy)
accuracy <- seq(0.001, 0.999, length.out = 500)
# Assume symmetric test: P(T|D) = accuracy, P(T|Dc) = 1 - accuracy

bayes_prob <- function(acc, prev = 0.01) {
  p_t <- acc * prev + (1 - acc) * (1 - prev)
  (acc * prev) / p_t
}

p_disease_given_pos <- bayes_prob(accuracy)

df_bayes <- data.frame(accuracy = accuracy * 100,
                       prob_disease = p_disease_given_pos * 100)

ggplot(df_bayes, aes(x = accuracy, y = prob_disease)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_vline(xintercept = 95, col = "red", linetype = "dashed") +
  annotate("text", x = 95, y = 50,
           label = paste0("95% accurate\nâ‰ˆ ",
                          round(bayes_prob(0.95) * 100, 1), "% of positives have disease"),
           hjust = -0.05, color = "red", size = 3.5) +
  labs(
    x = "Test Accuracy (%)",
    y = "P(Disease | Positive Test) (%)",
    title = "Bayes' Theorem: Effect of Test Accuracy on PPV",
    subtitle = "Disease prevalence = 1%"
  ) +
  theme_minimal()
```

> **What to notice:** The test needs to be **extremely** accurate
> (>99%) before a positive result becomes convincing. At 95% accuracy,
> most positive results are false alarms!

---

# 10. Conditional Probability Exercises

## Exercise 1: Given probabilities

We're given:

- $P(A) = 0.2$
- $P(B) = 0.4$
- $P(A \cap B) = 0.1$

We want to compute $P(A \mid B)$, $P(B \mid A)$, and check whether A
and B are independent.

Recall the formulas:

- $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$ â€” "Of the times B happens,
  how often does A also happen?"
- $P(B \mid A) = \frac{P(A \cap B)}{P(A)}$ â€” "Of the times A happens,
  how often does B also happen?"
- A and B are independent if $P(A \cap B) = P(A) \times P(B)$

```{r conditional_probability}
P_A       <- 0.2
P_B       <- 0.4
P_A_and_B <- 0.1

# Conditional probabilities
P_A_given_B <- P_A_and_B / P_B
P_B_given_A <- P_A_and_B / P_A

cat("P(A | B) =", P_A_given_B, "\n")
cat("P(B | A) =", P_B_given_A, "\n")

# Independence check: A and B are independent iff P(A âˆ© B) = P(A) * P(B)
cat("\nIndependence check:\n")
cat("P(A) * P(B) =", P_A * P_B, "\n")
cat("P(A âˆ© B)   =", P_A_and_B, "\n")
cat("Are A and B independent?", ifelse(P_A * P_B == P_A_and_B, "YES", "NO"), "\n")
cat("Since", P_A * P_B, "â‰ ", P_A_and_B, ", A and B are NOT independent.\n")
```

### Answers

- $P(A \mid B) = \frac{0.1}{0.4} = 0.25$ â€” Given B occurred, there's a
  25% chance A also occurred.
- $P(B \mid A) = \frac{0.1}{0.2} = 0.5$ â€” Given A occurred, there's a
  50% chance B also occurred.
- **Independence:** $P(A) \times P(B) = 0.08$, but $P(A \cap B) =
  0.1$. Since $0.08 \neq 0.1$, A and B are **NOT independent** â€”
  knowing one event changes the probability of the other.

---

## Exercise 2: UFO Sightings in the Outback

### The scenario

- $P(O \mid U) = 0.4$ â€” If you've seen a UFO, there's a 40% chance you
  live in the outback.
- $P(O \mid U^C) = 0.2$ â€” If you haven't seen a UFO, there's a 20%
  chance you live in the outback.
- $P(U) = 0.01$ â€” The overall probability of seeing a UFO is 1%.

**Question:** What is $P(U \mid O)$ â€” the probability of seeing a UFO,
*given* that you live in the outback?

### Applying Bayes' Theorem

First, find $P(O)$ using the Law of Total Probability:

$$P(O) = P(O \mid U) \cdot P(U) + P(O \mid U^C) \cdot P(U^C)$$

Then apply Bayes:

$$P(U \mid O) = \frac{P(O \mid U) \cdot P(U)}{P(O)}$$

```{r ufo_bayes}
# Let U = event of seeing a UFO, O = event of living in the outback
P_U         <- 0.01   # P(UFO)
P_O_given_U  <- 0.4   # P(Outback | UFO)
P_O_given_Uc <- 0.2   # P(Outback | No UFO)

# Law of total probability: P(O)
P_O <- P_O_given_U * P_U + P_O_given_Uc * (1 - P_U)
cat("P(Outback) =", P_O, "\n")

# Bayes' Theorem: P(UFO | Outback)
P_U_given_O <- (P_O_given_U * P_U) / P_O
cat("P(UFO | Outback) =", round(P_U_given_O, 6), "\n")
cat("Approximately", round(P_U_given_O * 100, 4), "%\n")
```

### Answer

$$P(U \mid O) = \frac{0.4 \times 0.01}{0.4 \times 0.01 + 0.2 \times 0.99} = \frac{0.004}{0.202} \approx 0.0198$$

If you live in the outback, there's approximately a **1.98%** chance
you've seen a UFO â€” slightly higher than the base rate of 1%, because
outback dwellers are more likely to report UFO sightings (perhaps
because of darker skies and less light pollution! ðŸ›¸).
