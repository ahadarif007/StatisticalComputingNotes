---
title: "Intro to R and Rstudio"
output: 
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float:
      collapsed: true
editor_options: 
  markdown: 
    wrap: 72
---

# Getting Started: R Basics for Beginners

If you've used Python before, R will feel partially familiar â€” but there
are some important differences. This section covers the essentials you
need to get going. **If you've never programmed before, don't worry** â€”
everything is explained step by step.

## Using the R Console (the "REPL")

The **console** is the panel (usually at the bottom of RStudio) where
you type commands and R responds with results. Here's what the prompts
mean:

-   **`>`** â€” R is ready and waiting for you to type something.
-   **`+`** â€” R thinks your previous line was incomplete (e.g., you
    opened a bracket but didn't close it). Either finish the expression
    or press **Escape** to cancel.
-   **Tab** â€” Press this key to auto-complete variable names, function
    names, and file paths. Very handy!
-   **Ctrl+Enter** (or **Cmd+Enter** on Mac) â€” Runs the current line (or
    highlighted code) from a script or R Markdown file.

## Key Differences: R vs Python

If you've used Python, here's a quick comparison:

| Python | R | Example |
|----|----|----|
| `obj.method(arg)` | `fun(obj, arg)` | R uses functions, not methods |
| `:` and indentation for blocks | `{ }` curly braces for blocks | `if (x > 0) { print("yes") }` |
| Indexing starts at **0** | Indexing starts at **1** | `x[1]` gets the *first* element in R |
| `[1, 2, 3]` | `c(1, 2, 3)` | `c()` stands for "combine" |
| `x.append("foo")` | `x <- c(x, "foo")` | R doesn't modify in-place |
| `=` for assignment | `<-` for assignment | `x <- 5` (the arrow "points" to the variable) |

> **Why `<-` instead of `=`?** Both *technically* work in R, but `<-` is
> the convention and they actually have slightly different behaviour in
> some situations. Just use `<-` and you'll be safe.

## How to Get Help

R has excellent built-in documentation:

-   **`?topic`** or **`help("topic")`** â€” Opens the help page for any
    function or topic. Example: `?mean`
-   **`example(fun)`** â€” Shows live examples of how a function is used.
    Try `example(plot)`!
-   **`help(package = "foo")`** â€” Shows all the functions in a package.
-   Don't be scared by functions with 20+ arguments â€” usually only 2 or
    3 are required. The rest have sensible defaults.

## Installing and Loading Packages

Packages are **add-on libraries** that extend R's capabilities (like pip
packages in Python):

-   **Install:** `install.packages("packagename")` â€” downloads the
    package (you only need to do this once).
-   **Load:** `library(packagename)` â€” makes the package available for
    your current session (you do this every time you restart R).
-   Unlike Python, you don't need to type `package.function()` â€” just
    call the function directly after loading the package. If two
    packages have functions with the same name, use
    `package::function()` to specify which one.

------------------------------------------------------------------------

# Statistical Computing â€“ Lab 1: R for Computational Statistics

## What This Lab Is About

In this module, **R isn't just a calculator** â€” it's a language for
writing **statistical algorithms**. Throughout the course, you'll use R
for things like:

-   **Likelihood-based inference** (estimating parameters from data)
-   **Simulation studies** (generating fake data to test methods)
-   **Numerical optimization** (finding the best-fitting model)
-   **Mixture models and MCMC** (advanced modelling techniques)

This lab teaches you the **core R skills** that everything else builds
on. Almost every concept here will come back in later weeks, so it's
worth getting comfortable now.

------------------------------------------------------------------------

## 1. Reproducibility and Basic Computation

### Why `set.seed()`?

Statistical computing uses **random numbers** a lot (for simulations,
sampling, etc.). But if your results change every time you run the code,
nobody can reproduce your work! `set.seed()` fixes the random number
generator to a specific starting point, so you get the **same "random"
numbers every time**.

```{r}
set.seed(123)

```

### Basic vector operations

A **vector** is just a list of numbers. It's the most fundamental data
structure in R. Here we create one and compute some basic statistics:

```{r}
x <- c(1, 2, 3, 4, 5)
mean(x)
sum(x)
sd(x)
```

-   **`mean(x)`** â€” the average: add up all the numbers and divide by
    how many there are.
-   **`sum(x)`** â€” adds up all the numbers.
-   **`sd(x)`** â€” the standard deviation: a measure of how spread out
    the numbers are.

### Computing the mean "by hand"

The sample mean is just
$\bar{x} = \frac{\text{sum of all values}}{\text{number of values}}$. We
can write that directly in R:

```{r}
sum(x) / length(x)
```

This gives the same answer as `mean(x)`. Later in the module, we'll see
that *how* you compute things can matter when numbers are very large or
very small (a topic called **numerical stability**).

### Your turn: Extreme numbers

What happens when we mix very large and very small numbers? Let's find
out. Below we create a vector with one huge number
(10,000,000,000,000,000) and three tiny ones (1, 2, 3), then compute the
mean three different ways:

-   **Method 1:** R's built-in `mean()`.
-   **Method 2:** Manual `sum(x) / length(x)`.
-   **Method 3:** A "compensated" approach that subtracts a reference
    value first to reduce rounding errors.

```{r}
x <- c(1e16, 1, 2, 3)

# Method 1: R's built-in mean
mean_builtin <- mean(x)

# Method 2: manual sum / length
mean_manual <- sum(x) / length(x)

# Method 3: compensated sum relative to first element
mean_compensated <- (sum(x - x[1]) + length(x) * x[1]) / length(x)

cat("Built-in mean:      ", mean_builtin, "\n")
cat("Manual mean:        ", mean_manual, "\n")
cat("Compensated mean:   ", mean_compensated, "\n")
cat("Difference (builtin - manual):", mean_builtin - mean_manual, "\n")
```

The difference looks like zero... but is it *really* zero? Let's display
all the decimal places to find out:

```{r}
options(digits = 22)
cat("Built-in mean:      ", mean_builtin, "\n")
cat("Manual mean:        ", mean_manual, "\n")
cat("Compensated mean:   ", mean_compensated, "\n")
cat("Difference (builtin - manual):", mean_builtin - mean_manual, "\n")
options(digits = 7)  # reset to default
```

> **Takeaway:** Computers store numbers with limited precision (about
> 15â€“16 significant digits). When you mix very large and very small
> numbers, the small ones can get "lost" in the rounding. This is why
> numerical methods matter!

------------------------------------------------------------------------

## 2. Vectors, Indexing, and Logical Operations

### What is a vector?

A **vector** is R's most basic data structure â€” simply an ordered
collection of values of the same type (all numbers, or all text, etc.).
Almost everything in R operates on vectors.

```{r}
x <- c(10, 11, 12)
names(x) <- c("first", "second", "third")
```

Here we created a vector with three numbers and gave each one a
**name**. Names are optional but make code more readable.

### Extracting elements (indexing)

You can pull out specific elements using square brackets `[ ]`:

```{r}
x[1:2]
x[x > 10]
x[c("first", "third")]
```

-   **`x[1:2]`** â€” Gets elements 1 and 2 (remember: R counts from 1, not
    0!).
-   **`x[x > 10]`** â€” Gets only the elements greater than 10. This is
    called **logical indexing** â€” R checks each element, keeps the ones
    where the condition is `TRUE`, and drops the rest.
-   **`x[c("first", "third")]`** â€” Gets elements by their names.

> **Logical indexing** is one of the most useful tricks in R. You'll use
> it constantly for filtering data.

### Your turn: Working with random numbers

The function `rnorm(n)` generates `n` random numbers from a Normal (bell
curve) distribution. If you're unsure what a function does, type
`?rnorm` in the console to see its help page.

```{r}
# Create a numeric vector of length 10 using rnorm()
set.seed(42)
v <- rnorm(10)
v
```

```{r}
# Extract only the positive values using logical indexing
v_positive <- v[v > 0]
v_positive
```

```{r}
# Replace all negative values with 0
v_nonneg <- v
v_nonneg[v_nonneg < 0] <- 0
v_nonneg
```

------------------------------------------------------------------------

## 3. From Vectors to Data Frames

### What is a data frame?

A **data frame** is R's version of a spreadsheet â€” it stores data in
rows and columns. Each column is a vector, and all columns must have the
same number of rows. This is how you'll store most datasets.

```{r}
students <- data.frame(
  name = c("Alice", "Bob", "Charlie", "Dana"),
  age = c(22, 23, 24, NA),
  grade = c("A", "B", "A", "B")
)

students
summary(students)
```

### Dealing with missing values (`NA`)

In R, **`NA`** means "Not Available" â€” i.e., the value is missing.
Dana's age is `NA` above. If you try to compute the mean of a column
with `NA`s, R will return `NA` (because it doesn't know the missing
value). To tell R "just ignore the missing ones", use `na.rm = TRUE`:

```{r}
mean(students$age, na.rm = TRUE)
```

> **`na.rm`** stands for "NA remove". You'll use this argument all the
> time.

### Adding new columns using a mapping

Sometimes you want to convert categories to numbers (e.g., grades to
numeric scores). You can do this by creating a **named vector** that
acts like a lookup table:

```{r}
# Add a new column converting grades to numeric values (A = 70, B = 60)
gmap <- c(A = 70, B = 60)
students$grade_numeric <- gmap[students$grade]
students
```

**How this works:** `gmap` is a named vector where `A` maps to 70 and
`B` maps to 60. When you write `gmap[students$grade]`, R looks up each
student's grade in the mapping and returns the corresponding number.

### Summarizing by group

The `aggregate()` function lets you compute a summary (like the mean)
**separately for each group**:

```{r}
# Compute the average age for each grade
aggregate(age ~ grade, data = students, FUN = mean, na.rm = TRUE)
```

> **Reading the formula:** `age ~ grade` means "break `age` into groups
> defined by `grade`, and apply the function to each group."

------------------------------------------------------------------------

## 4. Some Probability

### Why do we care about probability in R?

Many things we'll do later in this module â€” likelihood functions,
simulations, mixture models, MCMC â€” are built directly on probability.
This section lets you practise probability calculations **using R code**
rather than just pen and paper.

### Sample Spaces and Events

When we roll a fair die, the **sample space** $\Omega$ is the set of all
possible outcomes:

```{r}
omega <- 1:6
omega
```

Now let's define two **events** (subsets of the sample space):

-   **Event A:** The outcome is **even** (2, 4, or 6)
-   **Event B:** The outcome is **greater than 3** (4, 5, or 6)

```{r}
# here's a hint to get you started
omega %%2 == 0

A <- omega[omega %% 2 == 0]   # even outcomes: 2, 4, 6
B <- omega[omega > 3]          # outcomes > 3: 4, 5, 6
cat("A:", A, "\n")
cat("B:", B, "\n")
```

> **What's `%%`?** It's the **modulo** (remainder) operator.
> `omega %% 2 == 0` checks which numbers are divisible by 2 (i.e.,
> even).

### Computing probabilities

For a fair die, the probability of any event is:

$$P(\text{event}) = \frac{\text{number of outcomes in the event}}{\text{total number of outcomes}}$$

We also need:

-   **Intersection** $A \cap B$ â€” outcomes that are in *both* A and B. R
    function: `intersect(A, B)`
-   **Union** $A \cup B$ â€” outcomes that are in A *or* B (or both). R
    function: `union(A, B)`

```{r}
n <- length(omega)

P_A       <- length(A) / n
P_B       <- length(B) / n
P_A_and_B <- length(intersect(A, B)) / n
P_A_or_B  <- length(union(A, B)) / n

cat("P(A)       =", P_A, "\n")
cat("P(B)       =", P_B, "\n")
cat("P(A âˆ© B)   =", P_A_and_B, "\n")
cat("P(A âˆª B)   =", P_A_or_B, "\n")
```

### Verifying the Addition Rule

There's a fundamental rule in probability:

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

We subtract $P(A \cap B)$ because outcomes in *both* events would
otherwise be counted twice. Let's check this numerically:

```{r}
lhs <- P_A_or_B
rhs <- P_A + P_B - P_A_and_B
cat("P(A âˆª B)              =", lhs, "\n")
cat("P(A) + P(B) - P(Aâˆ©B) =", rhs, "\n")
cat("Equal?", isTRUE(all.equal(lhs, rhs)), "\n")
```

### Conditional Probability

**Conditional probability** answers: "If I already know that event B
happened, what's the probability of event A?"

The formula is:

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$

In words: "out of all the outcomes where B is true, what fraction also
satisfies A?"

#### Your turn

```{r}
# Compute P(A | B)
P_A_given_B <- P_A_and_B / P_B
cat("P(A | B) =", P_A_given_B, "\n")
```

```{r}
# Compute P(B | A)
P_B_given_A <- P_A_and_B / P_A
cat("P(B | A) =", P_B_given_A, "\n")
```

#### Are P(A \| B) and P(B \| A) the same?

**No!** P(A\|B) = 2/3 â‰ˆ 0.667, but P(B\|A) = 1/2 = 0.5. They are
different.

**Why?** Because they ask different questions:

-   P(A\|B) asks: "Given that the outcome is greater than 3, what's the
    chance it's also even?"
-   P(B\|A) asks: "Given that the outcome is even, what's the chance
    it's also greater than 3?"

The denominators are different (P(B) â‰  P(A)), so the answers are
different. Conditional probability is **not symmetric**.

### Independence of Events

Two events are **independent** if knowing that one occurred gives you
**no information** about whether the other occurred. Mathematically:

$$\text{A and B are independent if } P(A \cap B) = P(A) \times P(B)$$

Let's check whether our events A and B are independent:

```{r}
product_P_A_P_B <- P_A * P_B
cat("P(A âˆ© B)   =", P_A_and_B, "\n")
cat("P(A) * P(B) =", product_P_A_P_B, "\n")
cat("Independent?", isTRUE(all.equal(P_A_and_B, product_P_A_P_B)), "\n")
```

#### Does knowing B occurred change the probability of A?

**Yes!** P(A\|B) = 2/3, but P(A) = 1/2. Since P(A\|B) â‰  P(A), knowing B
occurred *does* change the probability of A, confirming the events are
**dependent**.

> **Key insight:** If events were independent, we'd have P(A\|B) = P(A)
> â€” knowing B wouldn't change anything about A. When P(A\|B) â‰  P(A), the
> events are dependent.

### Conditional Probability from a Table

Real-world data often comes in tables. Here's one that records smoking
status and disease status for 300 people:

|            | Disease | No Disease |
|------------|---------|------------|
| Smoker     | 30      | 70         |
| Non-smoker | 20      | 180        |

Let's store this as a **matrix** in R and compute some conditional
probabilities:

```{r}
smoking <- matrix(c(30, 70, 20, 180), nrow = 2, byrow = TRUE)
rownames(smoking) <- c("Smoker", "Non-smoker")
colnames(smoking) <- c("Disease", "No Disease")
smoking
```

Now we compute three probabilities:

1.  **P(Disease)** â€” What fraction of ALL people have the disease?
2.  **P(Disease \| Smoker)** â€” Of the smokers, what fraction have the
    disease?
3.  **P(Smoker \| Disease)** â€” Of the diseased people, what fraction are
    smokers?

```{r}
total <- sum(smoking)

# 1. P(Disease)
P_disease <- sum(smoking[, "Disease"]) / total
cat("P(Disease)           =", P_disease, "\n")

# 2. P(Disease | Smoker)
P_disease_given_smoker <- smoking["Smoker", "Disease"] / sum(smoking["Smoker", ])
cat("P(Disease | Smoker)  =", P_disease_given_smoker, "\n")

# 3. P(Smoker | Disease)
P_smoker_given_disease <- smoking["Smoker", "Disease"] / sum(smoking[, "Disease"])
cat("P(Smoker | Disease)  =", P_smoker_given_disease, "\n")
```

#### Are Smoking and Disease independent?

```{r}
P_smoker <- sum(smoking["Smoker", ]) / total

P_disease_and_smoker <- smoking["Smoker", "Disease"] / total
expected_if_indep    <- P_smoker * P_disease

cat("P(Smoker âˆ© Disease)       =", P_disease_and_smoker, "\n")
cat("P(Smoker) Ã— P(Disease)   =", expected_if_indep, "\n")
cat("Independent?", isTRUE(all.equal(P_disease_and_smoker, expected_if_indep)), "\n")
# They are not equal, so smoking and disease are NOT independent.
```

> **Interpretation:** P(Disease \| Smoker) = 0.30 but P(Disease) =
> 0.167. Smokers have a *much* higher disease rate than the overall
> population. Smoking and disease are clearly **not** independent.

------------------------------------------------------------------------

## 5. Visualisation as Statistical Thinking

### Why visualise?

Visualisation isn't just for making pretty pictures â€” it's often the
**first step** in understanding data. A histogram, for example, shows
you the *shape* of your data's distribution.

```{r}
data <- rnorm(500, mean = 50, sd = 10)

hist(data, probability = TRUE,
     main = "Histogram as a Density Estimate",
     xlab = "Value")
lines(density(data))
```

> **What's happening here:** `rnorm(500, mean = 50, sd = 10)` generates
> 500 random numbers from a Normal distribution centred at 50. The
> histogram shows the shape, and `density(data)` draws a smooth curve
> through it. The `probability = TRUE` argument makes the y-axis show
> *density* instead of counts, so the histogram and density curve are on
> the same scale.

### Effect of sample size

What happens when we have more data? The histogram gets smoother and
follows the true bell curve more closely:

```{r}
# Change the sample size and observe how the histogram and density change
set.seed(7)
data_large <- rnorm(5000, mean = 50, sd = 10)
hist(data_large, probability = TRUE,
     main = "Histogram with n = 5000",
     xlab = "Value")
lines(density(data_large), col = "blue", lwd = 2)
# With a larger sample the histogram follows the true normal density much more
# closely and the density curve is smoother.
```

### Effect of bin width

The number of bars (bins) in a histogram matters a lot:

-   **Too few bins** (wide bars) â€” Over-smooths the data. You lose
    detail.
-   **Too many bins** (narrow bars) â€” Under-smooths the data. You see
    noise instead of the true pattern.

```{r}
# Try a different bin width and comment on the effect
set.seed(7)
data_bw <- rnorm(500, mean = 50, sd = 10)

par(mfrow = c(1, 2))

hist(data_bw, probability = TRUE, breaks = 5,
     main = "Wide bins (breaks = 5)", xlab = "Value")
lines(density(data_bw), col = "red", lwd = 2)

hist(data_bw, probability = TRUE, breaks = 50,
     main = "Narrow bins (breaks = 50)", xlab = "Value")
lines(density(data_bw), col = "red", lwd = 2)

par(mfrow = c(1, 1))
# Wide bins over-smooth: we lose detail about the shape of the distribution.
# Narrow bins under-smooth: the histogram becomes spiky and we may be
# fitting noise rather than signal. A moderate bin width balances bias and
# variance â€” a theme that recurs in kernel density estimation.
```

> **Takeaway:** Choosing the right bin width is a **bias-variance
> trade-off** â€” a concept that comes up again and again in statistics.
> We'll revisit this formally when we study kernel density estimation.

------------------------------------------------------------------------

## 6. Lists: Storing Complex Objects

### What is a list?

A **list** is like a container that can hold **anything** â€” vectors,
data frames, even other lists. Unlike vectors (which must contain all
the same type), lists can mix numbers, text, dates, and more.

**Why do we care?** In R, almost every function that fits a model (like
`lm()`, `t.test()`, etc.) returns its results as a list. So
understanding lists is essential for working with statistical output.

```{r}
l <- list(
  numbers = x,
  summary = c(mean = mean(x), sd = sd(x)),
  metadata = list(source = "simulation", date = Sys.Date())
)

str(l)
```

> **`str()`** stands for "structure" â€” it shows you what's inside an
> object. Very useful for exploring lists!

------------------------------------------------------------------------

## 7. Reading and Writing Data

### Where is R looking for files?

R has a "working directory" â€” the folder it looks in when you ask it to
open or save files. You can check the current working directory with:

```{r}
getwd()
```

> **Heads up:** The working directory for your R Markdown file might be
> different from the one in the console below. This can cause confusion!
> The R Markdown file usually uses its own folder as the working
> directory.

### Setting up a data folder

It's good practice to organise your files. Let's create a subfolder for
our data:

```{r}
dir.create("lab0data", showWarnings = FALSE)
setwd("lab0data")
```

> **Note:** You might see a warning about `setwd()`. That's because
> inside R Markdown, changing the working directory only affects the
> current code chunk. A more reliable approach is to use the `here`
> package, which always gives paths relative to your project folder:

```{r}
library(here)
```

```{r}
file_path <- here("lab0data", "mydata.csv")
file_path
```

### Reading a CSV file

CSV (Comma-Separated Values) files are the most common way to store
datasets. `read.csv()` loads them into a data frame:

```{r}
housing <- read.csv("hp1602.csv")
str(housing)
```

### Saving and loading R objects

You can save any R object to a file and load it back later. This is
useful for **long-running analyses** â€” you don't want to re-run
everything from scratch if R crashes!

-   **`saveRDS()`** â€” saves a single R object to a file.
-   **`readRDS()`** â€” loads it back.

```{r}
saveRDS(housing, "housing.rds")
housing2 <- readRDS("housing.rds")
```

### Your turn: Explore the data

```{r}
# Inspect the column names and basic summaries of the housing data
cat("Column names:\n")
print(names(housing))

cat("\nDimensions:", nrow(housing), "rows x", ncol(housing), "columns\n")

summary(housing)
```

------------------------------------------------------------------------

## 8. Iteration and Vectorised Computation

### Two ways to repeat a calculation

In statistics, we often need to apply the same calculation to many
columns, many groups, or many simulated datasets. R gives us two
approaches:

1.  **Loops** (`for`) â€” familiar from Python, but often slower in R.
2.  **Vectorised functions** (`sapply`, `apply`, etc.) â€” R's preferred
    way, usually faster and more concise.

Here's `sapply()` computing the variance of every country column in the
housing data:

```{r}
housingVar <- sapply(housing[, -(1:3)], var)
housingVar
```

> **What's `housing[, -(1:3)]`?** It means "all columns *except* the
> first three." The minus sign removes columns by position.

### Your turn: Loop vs sapply

Let's compute the mean HPI (House Price Index) for each country, first
with a loop, then with `sapply()`.

**The loop approach:**

-   `unique()` gives you the distinct values in a column.
-   `numeric(n)` creates an empty vector of length `n` to store results.
-   `seq_along()` generates indices 1, 2, 3, ... matching the length of
    the input.

```{r}
# Compute the mean HPI for each country using a loop (loop over the countries)
countries <- names(housing)[-(1:3)]   # country column names

mean_hpi_loop <- numeric(length(countries))
names(mean_hpi_loop) <- countries

for (i in seq_along(countries)) {
  mean_hpi_loop[i] <- mean(housing[[countries[i]]], na.rm = TRUE)
}

mean_hpi_loop
```

**The `sapply()` approach** (same result, one line of code!):

```{r}
# Repeat the computation using sapply()
mean_hpi_sapply <- sapply(housing[, -(1:3)], mean, na.rm = TRUE)
mean_hpi_sapply

# Verify the two approaches give the same result
all.equal(mean_hpi_loop, mean_hpi_sapply)
```

> **Takeaway:** `sapply()` is more concise and often faster than a loop.
> Use it when you want to apply the same function to every element of a
> list or every column of a data frame.

------------------------------------------------------------------------

## 9. Splitâ€“Applyâ€“Combine

### A powerful pattern for grouped data

Many data analysis tasks follow this three-step pattern:

1.  **Split** the data into groups (e.g., by country, by treatment
    group).
2.  **Apply** a function to each group (e.g., compute the mean).
3.  **Combine** the results back together.

First, let's reshape the housing data from **wide format** (one column
per country) to **long format** (one row per country-observation pair):

```{r}
housing_long <- reshape(
  housing,
  varying = names(housing)[-(1:3)],
  v.names = "hpi",
  timevar = "country",
  times = names(housing)[-(1:3)],
  direction = "long"
)
```

Now we can compute the average HPI by country using `tapply()`:

```{r}
tapply(housing_long$hpi, housing_long$country, mean)
```

> **How `tapply()` works:** It takes a numeric vector
> (`housing_long$hpi`), splits it into groups defined by a factor
> (`housing_long$country`), applies a function (`mean`) to each group,
> and returns the results.

------------------------------------------------------------------------

## 10. Writing Your Own Functions

### Why write functions?

Functions let you **package up** a piece of logic so you can reuse it.
Instead of copying and pasting the same code over and over, you write it
once as a function and then call it whenever you need it.

**The basic structure:**

``` r
my_function <- function(argument1, argument2) {
  # do something
  return(result)
}
```

Here's a simple function that returns the mean and standard deviation of
a vector:

```{r}
meansd <- function(x) {
  c(mean = mean(x), sd = sd(x))
}
```

Apply it across groups:

```{r}
tapply(housing_long$hpi, housing_long$country, meansd)
```

### Your turn: Improve the function

Let's add the sample size (`n`) to the output, and make it handle
missing values (`NA`s) gracefully:

```{r}
# Modify meansd() to also return the sample size
meansd <- function(x) {
  x_clean <- x[!is.na(x)]
  c(mean = mean(x_clean), sd = sd(x_clean), n = length(x_clean))
}

# Test it
meansd(c(1, 2, 3, NA, 5))

# Apply across countries
tapply(housing_long$hpi, housing_long$country, meansd)
```

> **What's `!is.na(x)`?** It means "NOT missing". `is.na(x)` returns
> `TRUE` for each `NA` value, and `!` flips it. So `x[!is.na(x)]` keeps
> only the non-missing values.

------------------------------------------------------------------------

## 11. Simulation as a Statistical Tool

### What is simulation?

**Simulation** means generating fake data from a known model, then
studying what happens. It's one of the most powerful tools in modern
statistics because it lets you answer questions like:

-   "How reliable is this estimate?" (by repeating the experiment
    thousands of times)
-   "Does my method work correctly?" (by testing it on data where you
    know the true answer)

### The sampling distribution of the mean

If we take a sample of 50 numbers from a Normal distribution and compute
the mean, we'll get *some* number. If we repeat this 1000 times, those
1000 means form a **sampling distribution** â€” it tells us how much the
sample mean varies from sample to sample.

```{r}
simulate_mean <- function(n) mean(rnorm(n))

means <- replicate(1000, simulate_mean(50))
hist(means, probability = TRUE)
lines(density(means))
```

> **`replicate(1000, ...)`** runs the expression inside it 1000 times
> and collects the results. It's like a super-compact loop.

### The Central Limit Theorem in action

What happens if we change the sample size? The **Central Limit Theorem**
says that as the sample size increases, the sampling distribution of the
mean becomes:

-   More **concentrated** around the true mean (less spread)
-   More **bell-shaped** (even if the original data isn't normal!)

```{r}
# Change the sample size n and observe the effect on the sampling distribution
set.seed(123)

par(mfrow = c(1, 3))

for (n in c(5, 50, 500)) {
  sim_means <- replicate(1000, simulate_mean(n))
  hist(sim_means, probability = TRUE,
       main = paste("n =", n),
       xlab = "Sample mean",
       xlim = c(-2, 2))
  lines(density(sim_means), col = "steelblue", lwd = 2)
}

par(mfrow = c(1, 1))
# As n increases the distribution of the sample mean becomes more
# tightly concentrated around 0 (the true mean) and more bell-shaped,
# illustrating the Central Limit Theorem in action.
```

> **What to notice:** With n = 5, the sample means are all over the
> place (spread from âˆ’1 to +1). With n = 500, they're tightly clustered
> around 0 (the true mean). This is the CLT in action!

------------------------------------------------------------------------

## Exercises

These exercises bring together all the skills from the lab. Each one
builds on what you've learned above.

1.  **Compute the correlation** between US and Canada HPI.
2.  **Use simulation** (bootstrapping) to estimate how uncertain that
    correlation is.
3.  **Write a function** that returns the minimum, median, and maximum
    of a numeric vector.
4.  **Apply your function** to HPI values for each country.
5.  **(Advanced)** Plot the distribution of simulated correlations.

```{r}
# Exercise 1: Correlation between US and Canada HPI
cor_us_canada <- cor(housing$US, housing$Canada, use = "complete.obs")
cat("Correlation between US and Canada HPI:", cor_us_canada, "\n")
```

> **What's correlation?** It measures how strongly two variables move
> together. Values range from âˆ’1 (perfectly opposite) to +1 (perfectly
> together). A value near 0 means no linear relationship.

```{r}
# Exercise 2: Use simulation (bootstrap) to estimate uncertainty
set.seed(42)
n_boot <- 1000
boot_cors <- replicate(n_boot, {
  idx <- sample(nrow(housing), replace = TRUE)
  cor(housing$US[idx], housing$Canada[idx], use = "complete.obs")
})

cat("Bootstrap SE of correlation:", sd(boot_cors), "\n")
cat("95% CI: [", quantile(boot_cors, 0.025), ",",
    quantile(boot_cors, 0.975), "]\n")
```

> **What's bootstrapping?** We resample our data *with replacement*
> (some rows get picked multiple times, some are left out), compute the
> correlation on each resampled dataset, and repeat 1000 times. The
> spread of those 1000 correlations tells us how uncertain our estimate
> is. The 95% CI means: "we're 95% confident the true correlation falls
> in this range."

```{r}
# Exercise 3: Function returning min, median, max
min_med_max <- function(x) {
  x_clean <- x[!is.na(x)]
  c(min    = min(x_clean),
    median = median(x_clean),
    max    = max(x_clean))
}

# Quick test
min_med_max(c(3, 1, 4, 1, 5, 9, 2, 6))
```

```{r}
# Exercise 4: Apply to HPI values for each country
tapply(housing_long$hpi, housing_long$country, min_med_max)
```

```{r}
# Exercise 5 (Advanced): Plot the distribution of simulated correlations
hist(boot_cors, probability = TRUE,
     main = "Bootstrap Distribution of USâ€“Canada HPI Correlation",
     xlab = "Correlation",
     col  = "lightsteelblue",
     border = "white")
lines(density(boot_cors), col = "steelblue", lwd = 2)
abline(v = cor_us_canada, col = "red", lwd = 2, lty = 2)
legend("topleft",
       legend = c("Density", "Observed correlation"),
       col    = c("steelblue", "red"),
       lty    = c(1, 2), lwd = 2)
```

------------------------------------------------------------------------

## Summary

By the end of this lab you should feel comfortable with:

-   âœ… **Vectors and data frames** â€” R's core data structures
-   âœ… **Indexing and logical filtering** â€” extracting and subsetting
    data
-   âœ… **Probability in R** â€” computing P(A), P(A\|B), and checking
    independence
-   âœ… **Visualisation** â€” histograms, density plots, and the
    bias-variance trade-off in bin width
-   âœ… **Writing functions** â€” packaging up reusable logic
-   âœ… **Loops vs sapply** â€” two ways to repeat computations
-   âœ… **Simulation** â€” generating data to study statistical properties
    (like the CLT)

These skills form the **foundation** for everything else in this module.
You'll use them in every single future lab! ðŸš€
